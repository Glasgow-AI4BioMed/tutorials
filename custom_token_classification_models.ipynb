{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Colab Badge Link](https://img.shields.io/badge/open-in%20colab-blue)](https://colab.research.google.com/github/Glasgow-AI4BioMed/tutorials/blob/main/custom_token_classification_models.ipynb)\n",
        "\n",
        "# Creating a custom token classification model\n",
        "\n",
        "This notebook illustrates creating a custom Transformer model that is compatible with the [Huggingface trainer](https://huggingface.co/docs/transformers/main_classes/trainer). This model will use intermediate hidden states (so not the final hidden state) of a Transformer model for a token classification task."
      ],
      "metadata": {
        "id": "wfedic0l4G9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies\n",
        "\n",
        "If needed, you could install dependencies with the command below:\n",
        "\n",
        "```\n",
        "pip install transformers\n",
        "```"
      ],
      "metadata": {
        "id": "un2rAvC44OMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize some text\n",
        "\n",
        "We'll work with a single example. First we need a tokenizer:"
      ],
      "metadata": {
        "id": "C14RvAbC4E1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "t2_M1KsXEbPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's tokenize an example sentence."
      ],
      "metadata": {
        "id": "1k8omt4gERky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The quick brown fox jumps over the lazy dog.'\n",
        "\n",
        "encoded = tokenizer(text, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "encoded"
      ],
      "metadata": {
        "id": "vAo6UdJF39xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many tokens do we have? Looks like 15"
      ],
      "metadata": {
        "id": "uy5w0ydIEsLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded['input_ids'].shape"
      ],
      "metadata": {
        "id": "GDFLOKbzEbMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to make some dummy labels that will be our desired targets for our model. There are 15 tokens in the sequence and we need one for each token. The labels could correspond to `[0, B-DRUG, I-DRUG, etc]` for a biomedical NER task. Arbitrarily, let's say there are nine unique labels."
      ],
      "metadata": {
        "id": "fvsW0lDuEba9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 9"
      ],
      "metadata": {
        "id": "5ENCPF2k8Tf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create some labels randomly using [torch.randint](https://pytorch.org/docs/stable/generated/torch.randint.html)."
      ],
      "metadata": {
        "id": "AB0z6861E-xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "labels = torch.randint(low=0, high=num_labels, size=(1,15))\n",
        "labels.shape"
      ],
      "metadata": {
        "id": "_RjqZ1oE8Yah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For realism, some tokens shouldn't have labels, such as the `[CLS]` and `[SEP]` tokens used in many BERT models. In this case, they are at the beginning and end of this sequence. So to tell the model to ignore these (and not factor them into any calculations), you can set the labels to the special value of `-100`. The loss function that we'll use later ([CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)) knows that -100 denotes a token that should be ignored."
      ],
      "metadata": {
        "id": "OnCtgVn-FVm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels[0,0] = -100\n",
        "labels[0,-1] = -100"
      ],
      "metadata": {
        "id": "IJlcIeAlFTvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally what do our made-up labels look like?"
      ],
      "metadata": {
        "id": "4eLhLJkmFEec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "id": "Iigzaz-98T1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examining the AutoModelForTokenClassification"
      ],
      "metadata": {
        "id": "_okImOj64ji2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got some tokenized text and some made-up labels, let's see what happens when we put them through a standard `AutoModelForTokenClassification`. Our eventual model should give a similar output type as this.\n",
        "\n",
        "Let's create a `AutoModelForTokenClassification` and pass in the number of labels to be predicted."
      ],
      "metadata": {
        "id": "r1nopeWpGAhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)"
      ],
      "metadata": {
        "id": "cZZJGsC_34Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normally, you would then fine-tune this model with data before using it. But let's just use it now and see the type of the output. The actual output will be nonsense as there hasn't been any fine-tuning.\n",
        "\n",
        "Practically, when a model is trained, it is given data along with the labels. So let's take the tokenized text and add in the labels."
      ],
      "metadata": {
        "id": "a2uWJFh5GY75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_with_labels = dict(encoded)\n",
        "encoded_with_labels['labels'] = labels"
      ],
      "metadata": {
        "id": "UB5BXW-O9BU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we pass in the tokenized text with the labels and let's examine what it returns"
      ],
      "metadata": {
        "id": "1whqFEsfGzKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**encoded_with_labels)"
      ],
      "metadata": {
        "id": "UImzqLZS4rOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, what is the type of the object returned?"
      ],
      "metadata": {
        "id": "s-HqWwnfG5zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(output)"
      ],
      "metadata": {
        "id": "A5qGTGET44kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a [TokenClassifierOutput](https://huggingface.co/docs/transformers/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput) which wraps various bits of information.\n",
        "\n",
        "Let's just print it out and see what it gives:"
      ],
      "metadata": {
        "id": "96w4PDHvG9aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "LQHCwExW8Iw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two important things in this object:\n",
        " - **loss**: This is the loss that the fine-tuning will try to minimise.\n",
        " - **logits**: This is the output of the whole model\n",
        "\n",
        "Let's examine each. First the logits is a [pytorch.tensor](https://pytorch.org/docs/stable/tensors.html). Let's see it's dimensions"
      ],
      "metadata": {
        "id": "wofd48P_HQlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output.logits.shape"
      ],
      "metadata": {
        "id": "rzbUSIoh5u-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dimensions are explained below:\n",
        "  - 1: We've only given a single input text\n",
        "  - 15: The length of the input sequence\n",
        "  - 9: The number of labels\n",
        "\n",
        "For our custom model, we will want to output a tensor of this same dimension for the same input: `[1, 15, 9]`\n",
        "\n",
        "There is a score for each of the possible nine labels. Let's see the scores for the first token in the sequence:"
      ],
      "metadata": {
        "id": "VTvruT9NHv2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output.logits[0,0,:]"
      ],
      "metadata": {
        "id": "a4NWGtNb7ds-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could use `.argmax` to find the label that has the highest score. We don't need to do that here.\n",
        "\n",
        "As an aside, these scores are often [softmaxxed](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) to get nice scores between 0 and 1."
      ],
      "metadata": {
        "id": "aYIEFByiIAKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "softmax(output.logits[0,0,:].tolist())"
      ],
      "metadata": {
        "id": "S1mnv7gf9R2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other important output from this `AutoModelForTokenClassification` is the loss. This is a single number that the fine-tuning tries to minimise. It is calculated using the `logits` above when compared against the provided target `labels`."
      ],
      "metadata": {
        "id": "zgWbI8rtIc5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output.loss"
      ],
      "metadata": {
        "id": "I61oXb3n9N2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a custom TokenClassifierOutput\n",
        "\n",
        "Now let's say that we want to make our own model that can be used for TokenClassification but does things slightly differently.\n",
        "\n",
        "We might start off with a general-purpose `AutoModel` that doesn't have a final task-specific layer on it. If we wanted access to all the hidden layers, we can provide `output_hidden_states=True`."
      ],
      "metadata": {
        "id": "oMu3xApe4s-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(MODEL_NAME, output_hidden_states=True)"
      ],
      "metadata": {
        "id": "AbuMz2duEPm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can get the output of this model and rework it for what we need it to do."
      ],
      "metadata": {
        "id": "AbV6a05fJC-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**encoded)"
      ],
      "metadata": {
        "id": "gtrN2VFAE07F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get access to all the hidden states. Let's see how many and their dimensions."
      ],
      "metadata": {
        "id": "AQLibNHTJJ0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,hidden_state in enumerate(output.hidden_states):\n",
        "  print(i, hidden_state.shape)"
      ],
      "metadata": {
        "id": "Zlj6Wr7rE12M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 12 hidden layers in a standard BERT model, but there are 13 hidden states? Why? Well, we've got the input and output of all 12 layers which comes to 13 sets of context vectors. And all the context vectors are of dimension 768 which is common for standard BERT models.\n",
        "\n",
        "Now our target shape is `[1, 15, 9]`. One of the hidden layers is almost there, but we need to make it a bit smaller. For this, we can use a fully-connected linear layer to go from 768 down to 9:"
      ],
      "metadata": {
        "id": "b4oLSGaxJcyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "linear = torch.nn.Linear(768, num_labels)"
      ],
      "metadata": {
        "id": "-OboPx513s_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we apply the linear layer to the final hidden state, we get the logits with the desired shape."
      ],
      "metadata": {
        "id": "3ryYSTksKOPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = linear(output.hidden_states[-1])\n",
        "logits.shape"
      ],
      "metadata": {
        "id": "uzvyhRQr6KT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But we could also apply it to one of the intermediate hidden states!"
      ],
      "metadata": {
        "id": "IY4wM5q8KVUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = linear(output.hidden_states[5])\n",
        "logits.shape"
      ],
      "metadata": {
        "id": "-3ukCpv_60La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the moment, the linear layer is not fine-tuned, so the output logits would be meaningless. But with fine-tuning, these logits could give us the scores for each of the nine labels, with the highest score being the predicted label for that token.\n",
        "\n",
        "To effectively train it, we need to calculate the loss between provided labels and the model's current logits for that input. Then the training process can slowly move the logits towards the desired labels. So how do we calculate the loss?\n",
        "\n",
        "First, remember what the labels look like. We've got one input sequence and an integer representing the labels for each of the fifteen tokens."
      ],
      "metadata": {
        "id": "nL7TJuewKZRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels.shape"
      ],
      "metadata": {
        "id": "sNxhq6Zc78PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to calculate the loss, we use [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) which is used for multi-class classification problems. It expects two inputs:\n",
        "\n",
        "- The logits in the shape of (sample_count, num_labels)\n",
        "- The labels (as integers) in the shape (sample_count).\n",
        "\n",
        "We can use [.reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html) as below to adjust the shapes accordingly. And recall that CrossEntropyLoss will nicely ignore the tokens with `-100` labels as they shouldn't contribute to the loss."
      ],
      "metadata": {
        "id": "vDzNVaN2MHfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "loss = loss_func(logits.reshape(-1,num_labels), labels.reshape(-1))\n",
        "loss"
      ],
      "metadata": {
        "id": "b95d_IHv7hHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've calculated the logits and loss, we can create a `TokenClassifierOutput` object that encapsulates them. Now it looks like we have an output similar to `AutoModelForTokenClassification`."
      ],
      "metadata": {
        "id": "58SFUViTMxf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "\n",
        "TokenClassifierOutput(loss=loss, logits=logits)"
      ],
      "metadata": {
        "id": "vCsuDgf9_Oh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a custom model\n",
        "\n",
        "To actually use this custom approach, we need to wrap it up as a `torch.nn.Module`. The example class below takes the various steps from before and puts them into a single class"
      ],
      "metadata": {
        "id": "0gfAJQzg62Cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(torch.nn.Module):\n",
        "  def __init__(self, num_labels, hidden_layer):\n",
        "    super().__init__()\n",
        "    self.base_model = AutoModel.from_pretrained(MODEL_NAME, output_hidden_states = True)\n",
        "\n",
        "    self.num_labels = num_labels\n",
        "    self.hidden_layer = hidden_layer\n",
        "\n",
        "    self.linear = torch.nn.Linear(768, num_labels)\n",
        "    self.loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "    output = self.base_model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "\n",
        "    logits = self.linear(output.hidden_states[self.hidden_layer])\n",
        "\n",
        "    loss = None\n",
        "    if labels is not None: # If we're provided with labels, use them to calculate the loss\n",
        "      loss = self.loss_func(logits.reshape(-1,self.num_labels), labels.reshape(-1))\n",
        "\n",
        "    return TokenClassifierOutput(loss=loss, logits=logits)"
      ],
      "metadata": {
        "id": "VEbIa7gUFCw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the above class works very similarly to the actual implementation for the `AutoModelForTokenClassification`. You can have a look at `BertModelForTokenClassification` in the [HuggingFace source code](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L1714)\n",
        "\n",
        "One key difference is that this implementation does not use [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) which may be beneficial.\n",
        "\n",
        "Now we can create a model and even select which hidden_layer to connect to the output (and thereby removing some final layers from the calculation)."
      ],
      "metadata": {
        "id": "IS5HlCUkND5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomModel(num_labels=num_labels, hidden_layer=5)"
      ],
      "metadata": {
        "id": "B0BCKEbE_nqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's pass in the tokenized text with the labels and see what we get"
      ],
      "metadata": {
        "id": "XILbUblONrjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**encoded_with_labels)\n",
        "output"
      ],
      "metadata": {
        "id": "JiHErUuvGAHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent. The [HuggingFace trainer](https://huggingface.co/docs/transformers/main_classes/trainer) can now be used to fine-tune the model on an appropriate dataset."
      ],
      "metadata": {
        "id": "IzavHjdgNxEi"
      }
    }
  ]
}