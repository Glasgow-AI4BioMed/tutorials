{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Colab Badge Link](https://img.shields.io/badge/open-in%20colab-blue)](https://colab.research.google.com/github/Glasgow-AI4BioMed/tutorials/blob/main/huggingface_tokenizer_tricks.ipynb)\n",
        "\n",
        "# Useful Tips & Tricks when using the HuggingFace tokenizer\n",
        "\n",
        "The tokenizer in the HuggingFace transformers library can do a lot of useful things. But it can be a bit of a mystery how to use some of them. Here are a bunch of examples of useful things you can do."
      ],
      "metadata": {
        "id": "ANUGWYg25nJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies\n",
        "\n",
        "If needed, you could install dependencies with the command below:\n",
        "\n",
        "```\n",
        "pip install transformers\n",
        "```"
      ],
      "metadata": {
        "id": "gicFtsFdFbZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic tokenization\n",
        "\n",
        "Here is a tokenizer with default settings (using PubMed BERT)"
      ],
      "metadata": {
        "id": "XS9sAyROwQNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")"
      ],
      "metadata": {
        "id": "HbGvCX0iwNbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "By default, it gives us `input_ids`, `token_type_ids` and `attention_mask`. We normally only focus on the `input_ids` which is the text tokenized and converted to the IDs in the vocabulary."
      ],
      "metadata": {
        "id": "eTv71Zav6Aam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = tokenizer(\"The quick brown fox jumped over the lazy dog\")\n",
        "tokenized"
      ],
      "metadata": {
        "id": "8haPNIRZw5zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quickly looking up a token ID\n",
        "\n",
        "The `.vocab` attribute is the dictionary of the tokenizer's vocabulary. You can use it to look up the token ID of individual tokens, which is sometimes useful."
      ],
      "metadata": {
        "id": "qhVxtLxWEI8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab['quick']"
      ],
      "metadata": {
        "id": "ah-hQl9SEGGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing text that has already been split into words\n",
        "\n",
        "You may have input text that has already been split into words/tokens. This could be from source files or from using another library like [Spacy](https://spacy.io). A lot of datasets come in the [CoNLL file format](https://simpletransformers.ai/docs/ner-data-formats/#text-file-in-conll-format) which has things split into words. The HuggingFace tokenizer can deal with text like this using the `is_split_into_words` argument."
      ],
      "metadata": {
        "id": "DRMcIkiZCwHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretokenized = ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
        "\n",
        "tokenized = tokenizer(pretokenized, is_split_into_words=True)\n",
        "tokenized"
      ],
      "metadata": {
        "id": "TyAxkSxKCwN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the tokens back from the IDs\n",
        "\n",
        "It can be helped to convert the `input_ids` given by the tokenizer back into tokens to see what the tokenizer has done. The `convert_ids_to_tokens` function can do that.\n",
        "\n",
        "Note how some of the words have been broken into smaller subwords with the `##` prefix to show that the token connects to the previous one. And we can also see the special tokens `[CLS]` and `[SEP]` that are added to the start and end of sequences by default."
      ],
      "metadata": {
        "id": "kUrtSyRf_HDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(tokenized['input_ids'])"
      ],
      "metadata": {
        "id": "0e3-Q1OXw9Js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Truncating text\n",
        "\n",
        "Transformer models have a maximum length of text (by count of tokens) that they can accept. For some models, this is 512 tokens. There are various ways to trim or truncate the input data to make sure it doesn't surpass that limit. The tokenizer has `truncation` and `max_length` arguments to do that."
      ],
      "metadata": {
        "id": "co7ksN45_Y4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = tokenizer(\"The quick brown fox jumped over the lazy dog\",\n",
        "                      truncation=True,\n",
        "                      max_length=512)"
      ],
      "metadata": {
        "id": "m9bqXPPCwg0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the token offsets in the original text\n",
        "\n",
        "You may often want to be able to go from the tokens back to the original text. For example, you may want to know which tokens relate to a particular span of text, e.g. a set of words that represent a named entity. The tokenizer has the `return_offsets_mapping` argument that adds an extra field `offset_mapping` to the tokenized output. It is a list of start and end offsets into the text.\n",
        "\n",
        "If you have annotations for the locations of named entities in some text, you can use the `offset_mapping` data to match the tokens. The [intervaltree library](https://pypi.org/project/intervaltree/) can be very helpful to make some clean code if there are a lot of annotations to account for."
      ],
      "metadata": {
        "id": "sZKcN-hp_sVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"The quick brown fox jumped over the lazy dog\"\n",
        "tokenized = tokenizer(input_text,\n",
        "                      return_offsets_mapping = True)\n",
        "\n",
        "tokenized['offset_mapping']"
      ],
      "metadata": {
        "id": "CMMrOamPwmP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a demonstration of iterating through the `offset_mapping` and getting the text from the source text."
      ],
      "metadata": {
        "id": "sEnb0pfJAGgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for start,end in tokenized['offset_mapping']:\n",
        "  print(f\"{start}\\t{end}\\t{input_text[start:end]}\")"
      ],
      "metadata": {
        "id": "dSi7LGQ4xNYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Pytorch tensors\n",
        "\n",
        "The `input_ids` are probably going to be passed into a deep learning network, so will need to be converted into a Pytorch tensor at some point. You can get the tokenizer to do it with `return_tensors='pt'` where `'pt'` refers to pytorch. You can ask for TensorFlow tensors as well. You may want to use this argument along with the truncation ones."
      ],
      "metadata": {
        "id": "GGg8vEeJAUkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = tokenizer(\"The quick brown fox jumped over the lazy dog\",\n",
        "                      return_tensors='pt')\n",
        "tokenized"
      ],
      "metadata": {
        "id": "aWy3YH8uxbt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding your own \"special\" tokens\n",
        "\n",
        "Tokenizers have a bunch of special tokens, such as the `[CLS]` and `[SEP]` added at the start and end of sequences and the `[MASK]` tag used in the masked learning pretraining. You can see all the ones that a tokenizer uses with `.all_special_tokens`."
      ],
      "metadata": {
        "id": "1CcGJbsgAvNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.all_special_tokens"
      ],
      "metadata": {
        "id": "NwQIsnOgzDAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may want to use your own special tokens for something, like tagging around particular entities. However, if you do that without setup, the tokenizer has never seen them before and will likely split these into separate tokens as below."
      ],
      "metadata": {
        "id": "it-bY9HAB8K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = tokenizer(\"The quick brown [E1]fox[/E1] jumped over the lazy [E2]dog[/E2]\")\n",
        "\n",
        "tokenizer.convert_ids_to_tokens(tokenized['input_ids'])"
      ],
      "metadata": {
        "id": "P7XNfAoUxvSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead, you can add your tokens to the tokenizer with `.add_tokens` as below."
      ],
      "metadata": {
        "id": "3Azy_3QnCIsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
        "\n",
        "tokenizer.add_tokens([\"[E1]\", \"[/E1]\", \"[E2]\", \"[/E2]\"])"
      ],
      "metadata": {
        "id": "-xgVUf73xv5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check if one of the special tokens is now in the tokenizer's vocab:"
      ],
      "metadata": {
        "id": "w9bRPp1bElqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab['[E1]']"
      ],
      "metadata": {
        "id": "oOyraYVgEiHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then when you run the same code again, the special tokens have been encoded properly and not split into multiple tokens."
      ],
      "metadata": {
        "id": "027GD3kzCRTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = tokenizer(\"The quick brown [E1]fox[/E1] jumped over the lazy [E2]dog[/E2]\")\n",
        "\n",
        "tokenizer.convert_ids_to_tokens(tokenized['input_ids'])"
      ],
      "metadata": {
        "id": "Rt0E7XvWyWp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use these new tokens in a model, you'll need to tell the model to adjust to the new number of tokens using `resize_token_embeddings` as below.\n",
        "\n",
        "However, the initial embeddings for these new tokens will be random, and the system will have no idea of their meaning. So end-to-end training may be required for whatever task you are focussed on to get the system to use them effectively."
      ],
      "metadata": {
        "id": "WD8X6WNbWw8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "model = AutoModel.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "jSMoxyDCWxVb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}