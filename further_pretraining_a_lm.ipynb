{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Colab Badge Link](https://img.shields.io/badge/open-in%20colab-blue)](https://colab.research.google.com/github/Glasgow-AI4BioMed/tutorials/blob/main/further_pretraining_a_lm.ipynb)\n",
        "\n",
        "# Further pretraining a language model and generating text with it\n",
        "\n",
        "This Colab demonstrates taking a pretrained language model (distilgpt2 in this case), pretraining it further using the HuggingFace Trainer and then at the end generating new text with it.\n",
        "\n",
        "The first part is largely based on the [HuggingFace language modeling tutorial](https://huggingface.co/docs/transformers/tasks/language_modeling)."
      ],
      "metadata": {
        "id": "7mrD64-jCI_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies\n",
        "\n",
        "If needed, you could install dependencies with the command below:\n",
        "\n",
        "```\n",
        "pip install transformers datasets accelerate\n",
        "```"
      ],
      "metadata": {
        "id": "qTPUqbwYCYH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further pretraining a language model\n",
        "\n",
        "The first part is using some new text to further pretrain the language model"
      ],
      "metadata": {
        "id": "Eq7XS2ndC0dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get text to further pretrain\n",
        "\n",
        "We'll download [Shakespeare sonnets](https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt) to further pretrain the language model on."
      ],
      "metadata": {
        "id": "F8XaqNkICcFW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs8VM88U-d7P"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we'll load it up and store it as one long string."
      ],
      "metadata": {
        "id": "GCoMptkaCofX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('shakespeare.txt') as f:\n",
        "  shakespeare = [ line.strip() for line in f ]\n",
        "  shakespeare = shakespeare[4:] # Skip the title\n",
        "  shakespeare = \" \".join(shakespeare)"
      ],
      "metadata": {
        "id": "LfvvWD4-_Uf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the beginning of that"
      ],
      "metadata": {
        "id": "PQXpdGBCG6H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare[:100]"
      ],
      "metadata": {
        "id": "RqI4lVvLG6MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the text\n",
        "\n",
        "Now we'll tokenize the text and convert it to token IDs. We'll use the `distilgpt2` model here. Notably we're stuck with the previously created tokenizer so if there are new interesting words in our new text, we are unable to adapt the tokenizer to deal with them well and it may split them up strangely."
      ],
      "metadata": {
        "id": "sDqguDq_CvYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")"
      ],
      "metadata": {
        "id": "aOWlfS0X-50G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance, Shakespeare uses the word 'honorificabilitudinitatibus' in Love’s Labour’s Lost. Maybe it would be important that the tokenizer deals with it gracefully. As we're building on an existing language model, we have to keep the already existing tokenizer (and cannot create a new one). Let's see how it does."
      ],
      "metadata": {
        "id": "wXj7xbLnDejg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize('honorificabilitudinitatibus')"
      ],
      "metadata": {
        "id": "z-9TpehzDcqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not great but probably doesn't matter too much here. This may be more important in domains such as biomedical text where there are a lot of uncommon words that a general-purpose tokenizer badly butchers into unhelpful subword tokens.\n",
        "\n",
        "Now let's tokenize our big bit of text."
      ],
      "metadata": {
        "id": "fT-b3dyYDvEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = tokenizer(shakespeare)"
      ],
      "metadata": {
        "id": "4W0QGv05_a-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It gives a warning about the tokenized text being far longer than the maximum sequence for this model (1024). We need to split it into blocks to be processed one at a time.\n",
        "\n",
        "There are a few key fields that we'll examine: `input_ids` and `attention_mask`.\n",
        "\n",
        "First, we've got the `input_ids` that are the numeric token identifiers and the `attention_mask` which is used to tell the Transformer to ignore any padding (which we shouldn't have here). Let's just see the `input_ids`"
      ],
      "metadata": {
        "id": "GraXsnJgEBmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized['input_ids'][:10]"
      ],
      "metadata": {
        "id": "lWHeZGH9ERxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check what those `input_ids` translate back to in text using the `.decode` function of the tokenizer."
      ],
      "metadata": {
        "id": "KQeuwchYHLZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([4863, 37063, 301, 8109, 356, 6227, 2620, 11, 1320, 12839])"
      ],
      "metadata": {
        "id": "S7Jx0cY8Gpca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What about the `attention_mask`? We don't have padding so this shouldn't show much. It should contain a `1` for tokens to pay attention to and `0` for tokens to ignore."
      ],
      "metadata": {
        "id": "_UheLJslFP0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized['attention_mask'][:10]"
      ],
      "metadata": {
        "id": "eU1Si_SEFP8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All `1`s. In fact, if we check the whole `attention_mask` there are only 1s as we have no padding and hence no tokens to ignore."
      ],
      "metadata": {
        "id": "sFWkplOFHd0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set(tokenized['attention_mask'])"
      ],
      "metadata": {
        "id": "FfYwGnAWGgcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the dataset into blocks\n",
        "\n",
        "How many tokens do we have in the whole corpus?"
      ],
      "metadata": {
        "id": "Ob_u76wmENES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_length = len(tokenized['input_ids'])\n",
        "total_length"
      ],
      "metadata": {
        "id": "5ByVNSTyEVko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As noted in a warning when we tokenized the text, it's too long to process in one go. We need to split it into chunks. Let's follow the [HuggingFace tutorial](https://huggingface.co/docs/transformers/tasks/language_modeling)'s choice of 128."
      ],
      "metadata": {
        "id": "VXnuWcUmEVFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128"
      ],
      "metadata": {
        "id": "2bvek8SGEWg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And for simplicity, we want our total length to be an exact multiple of the block size, so let's make that happen:"
      ],
      "metadata": {
        "id": "uKxNQSJ7Ezsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_length = (total_length // block_size) * block_size\n",
        "total_length"
      ],
      "metadata": {
        "id": "Iq5x1ZHpEWtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we block up the `input_ids` and `attention_mask` in `tokenized` into blocks of length 128"
      ],
      "metadata": {
        "id": "ieo6-I6oFbpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_blocks = {\n",
        "    k: [t[i : i + block_size] for i in range(0, total_length, block_size)] for k, t in tokenized.items()\n",
        "}"
      ],
      "metadata": {
        "id": "d15N7Px6ADBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many blocks have we got?"
      ],
      "metadata": {
        "id": "GdZke46LFll3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenized_blocks['input_ids'])"
      ],
      "metadata": {
        "id": "HXLaGYWWFlD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to be training using this data so we need to tell the system what the expected output is. In causal language modelling, we're doing next token prediction. Hence the tokens that are used as input are effectively the intended outputs as well. Practically, they are shifted over by one, so that the target output token for an input token is the next one (and not itself). But HuggingFace does that shift for us, and we just copy the `input_ids` in as a field called `labels` that the Trainer picks up."
      ],
      "metadata": {
        "id": "IRrx7uXwE8-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_blocks[\"labels\"] = tokenized_blocks[\"input_ids\"].copy()"
      ],
      "metadata": {
        "id": "D2D04zhBE9DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Dataset object\n",
        "\n",
        "Before we can start running this, we need to turn this data into a `Dataset` object that HuggingFace is happy to play with. We can use the `from_dict` function for that"
      ],
      "metadata": {
        "id": "Dyob-iu0GQXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "lm_dataset = Dataset.from_dict(tokenized_blocks)\n",
        "lm_dataset"
      ],
      "metadata": {
        "id": "ZE01hUe9A7bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And practically we want a training set and a validation set so that we can watch the various metrics to understand how well the model is training and generalizing. We can use the `.train_test_split` function of the `Dataset` object for this."
      ],
      "metadata": {
        "id": "Q2jUPLizGT4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm_dataset = lm_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
        "lm_dataset"
      ],
      "metadata": {
        "id": "OAk_KIAiGV8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training!\n",
        "\n",
        "Now we get ready to actually train the language model. First we set up a `DataCollatorForLanguageModeling` which does the nice job of moving data around and getting things in the right place and right form for our task (language modelling). We use `mlm=False` which tells it we are not doing a masked language modeling task, instead we are doing causal language modeling."
      ],
      "metadata": {
        "id": "zF3n94-HIboh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "bsFt6lbBARcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we load the pretrained `distilgpt2` model. This has already been trained on lots of text, and we're going to take it a bit further and trained it with the Shakespeare text."
      ],
      "metadata": {
        "id": "3m2ERfQKIynM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")"
      ],
      "metadata": {
        "id": "ybMN3gkIAk9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now for some actual training. Realistically, you would need to try many parameter settings, monitor the validation loss and decide the best set up. But for now, we'll just pick some values.\n",
        "\n",
        "**Importantly:** We haven't told HuggingFace anything about GPUs, but it will, by default, check if one is available and use it. This lab should have a GPU so should run quickly."
      ],
      "metadata": {
        "id": "IHA2NbslJFBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"notused\", # Use save_strategy=\"no\" to not dump out to file\n",
        "    save_strategy=\"no\", # We'll save the model ourselves at the end (but may want to when longer slower training)\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=3,\n",
        "    report_to=\"none\" # Let's not use wandb here\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_dataset[\"train\"],\n",
        "    eval_dataset=lm_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "I9c91EojAa6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yay, we've trained a model. It would be a good idea to try different hyperparameters (e.g. more epochs, different learning rate, etc) to see what extra performance can be achieved.\n",
        "\n",
        "Now we'll save the model to disk where it could be loaded in another process and used for generation. You could also use the `trainer.save_model` function here."
      ],
      "metadata": {
        "id": "LXetvTM-Ja_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"shakespeare_model\")"
      ],
      "metadata": {
        "id": "pMHImCigBeAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also save the tokenizer (though nothing has changed as we used an unchanged `distilgpt2` tokenizer)."
      ],
      "metadata": {
        "id": "ffxH4IWOLyMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"shakespeare_model\")"
      ],
      "metadata": {
        "id": "jZmkgHzZNL0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what the files look like. The important one is `config.json` that HuggingFace looks for when it tries to load a model."
      ],
      "metadata": {
        "id": "-jlll8OoNXw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls shakespeare_model"
      ],
      "metadata": {
        "id": "-w156WkdL3BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a language model for generating text\n",
        "\n",
        "We'll use a text generation pipeline now. We can either provide a specific model & tokenizer (which may be helpful if we need to do some custom things) or give it the name for it to load itself.\n",
        "\n",
        "Now let's do the longer way first where we loaded the model/tokenizer ourselves:\n",
        "\n",
        "**Importantly:** We do need to tell the pipeline to use the GPU here (with `device='cuda:0'`)"
      ],
      "metadata": {
        "id": "tTEGO8sRJ-TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# This loads the model and tokenizer from disk\n",
        "model = AutoModelForCausalLM.from_pretrained(\"shakespeare_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"shakespeare_model\") # Could also have loaded the `distilgpt2` tokenizer as it is the same\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device='cuda:0')"
      ],
      "metadata": {
        "id": "bEalpqj3JQ7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively the equivalent way of loading it is below by giving the name (`\"shakespeare_model\"`) that we saved it with earlier. HuggingFace will always search the local directory first for that model before going to the [Huggingface Hub](https://huggingface.co/docs/hub/index) and downloading it (if one there matches)."
      ],
      "metadata": {
        "id": "OlGZhuijsRZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=\"shakespeare_model\", device='cuda:0')"
      ],
      "metadata": {
        "id": "aZMGx2NusGhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can pass in some text to the text generation pipeline. We need to tell it how many extra tokens to generate with `max_new_tokens`."
      ],
      "metadata": {
        "id": "aN2UugtnN_w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"Is that a dagger which I see\", max_new_tokens=20)"
      ],
      "metadata": {
        "id": "_2ReLgDvN2fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also pass in a few sequences:"
      ],
      "metadata": {
        "id": "JRfJ9nKKOSMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "several_sequences = [\n",
        "    \"To be, or not to\",\n",
        "    \"All the world's a\",\n",
        "    \"A horse! a horse! my kingdom for a\",\n",
        "    \"Friends, Romans, countrymen, lend me your\"\n",
        "]\n",
        "\n",
        "generator(several_sequences, max_new_tokens=20)"
      ],
      "metadata": {
        "id": "_ZrcjXEdN8a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are various parameters for text generation. The defaults may be set in `model.generation_config` or fallback to the defaults in the [GenerationConfig documentation](https://huggingface.co/docs/transformers/v4.30.0/main_classes/text_generation).\n",
        "\n",
        "Let's examine setting a few of them manually. We'll explicitly ask for three possible sequences (using `num_return_sequences=3`) using sampling (`do_sample=True`) so that there is a random factor in generation. Sampling works well for making interesting text, but for experiments with a language model it is more typical to not use sampling."
      ],
      "metadata": {
        "id": "1w0O3JQfPJlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"Is that a dagger which I see\", max_new_tokens=20, do_sample=True, num_return_sequences=3)"
      ],
      "metadata": {
        "id": "X3xDx5_UTHUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate some text deterministically without sampling (which is often the approach for experiments on language models), use `do_sample=False`. This outputs the most likely token each time."
      ],
      "metadata": {
        "id": "4zgHDVxETxi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"Is that a dagger which I see\", max_new_tokens=20, do_sample=False)"
      ],
      "metadata": {
        "id": "W0FtlmiXTVaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also set `top_p` which is one factor to filter out less common tokens (and reduce the likelihood of it generating really odd looking text)."
      ],
      "metadata": {
        "id": "8VIfgHwuUBcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"Is that a dagger which I see\", max_new_tokens=20, do_sample=True, top_p=0.7)"
      ],
      "metadata": {
        "id": "7WBKaYU0O4QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or `temperature`. See [this page](https://lukesalamone.github.io/posts/what-is-temperature/) for more of an explanation. Low temperature makes it more deterministic, higher temperature makes it more \"creative\"."
      ],
      "metadata": {
        "id": "6vt55wEoUKKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"Is that a dagger which I see\", max_new_tokens=20, do_sample=True, temperature=0.1)"
      ],
      "metadata": {
        "id": "Zhcw9ULrPW51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You could also use `return_full_text=False` to only get the new generated text (instead of it all)."
      ],
      "metadata": {
        "id": "wTzgKv6cVmCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"Is that a dagger which I see\", max_new_tokens=20, do_sample=False, return_full_text=False)"
      ],
      "metadata": {
        "id": "PZOfodrqVmaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are lot of different parameters that can be explored for using sampling in text (with `do_sample=True`). However, there are plenty of scenarios where you don't want sampling. The parameters can be examined on the [GenerationConfig documentation page](https://huggingface.co/docs/transformers/v4.30.0/main_classes/text_generation)."
      ],
      "metadata": {
        "id": "vAmxa4M0Ufcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Reading\n",
        "\n",
        "HuggingFace provides a good [blog post](https://huggingface.co/blog/how-to-generate) about language generation that goes over many of the techniques including beam search. Note again that many of these techniques use sampling (and will be non-deterministic) which isn't always what is desired. It depends on the problem. There's also details of the different generation algorithms on [this page](https://huggingface.co/docs/transformers/v4.30.0/generation_strategies)."
      ],
      "metadata": {
        "id": "I6tXxCPFUgpq"
      }
    }
  ]
}