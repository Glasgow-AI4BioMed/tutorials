{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Colab Badge Link](https://img.shields.io/badge/open-in%20colab-blue)](https://colab.research.google.com/github/Glasgow-AI4BioMed/tutorials/blob/main/pytorch_training_loop_with_custom_hugging_face_model.ipynb)\n",
        "\n",
        "## Example of PyTorch Training Loop with Custom Hugging Face Model\n",
        "\n",
        "This Colab illustrates a PyTorch training loop for training a custom transformer model. This contrasts with the HuggingFace Trainer. Using your own loop can give you more control over how the training works and any reporting that you want."
      ],
      "metadata": {
        "id": "tgcKEAfANmtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset\n",
        "\n",
        "We'll use part of the [Stanford IMDB dataset](https://huggingface.co/datasets/stanfordnlp/imdb) to illustrate this. It is a dataset of movie reviews and a label of if they are positive or negative. We'll use Hugging Face's [datasets library](https://huggingface.co/docs/datasets/index) to download it. First, install the library:"
      ],
      "metadata": {
        "id": "DnRLGoD_MAAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets"
      ],
      "metadata": {
        "id": "02LCr-2mFdDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then load the imdb dataset:"
      ],
      "metadata": {
        "id": "vBf69v99OL_k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rV_aTuPFQGi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "imdb = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll pick a tiny part of it to use. Just the texts and labels for a few hundred examples:"
      ],
      "metadata": {
        "id": "K2j6LvvXONsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = imdb['train'][:500]['text']\n",
        "labels = imdb['train'][:500]['label']"
      ],
      "metadata": {
        "id": "ZecDjVuYFthz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example:"
      ],
      "metadata": {
        "id": "B9MfsnUCORrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels[0], texts[0]"
      ],
      "metadata": {
        "id": "j6OK_3ZwORxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we'll split them into training and validation sets to illustrate working with a training and validation set."
      ],
      "metadata": {
        "id": "s2rySHP8OVt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "texts_train, texts_val, labels_train, labels_val = train_test_split(texts, labels, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "h9UbnQOhGhFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing the text\n",
        "\n",
        "Next we need to preprocess that data. We'll use a `bert-base-uncased` model and tokenized them, while keeping track of the labels."
      ],
      "metadata": {
        "id": "SeFylXlkL7mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize(text, label):\n",
        "  tokenized = tokenizer(text, truncation=True, max_length=512, return_tensors='pt')\n",
        "  tokenized['label'] = torch.tensor(label).reshape(1,1)\n",
        "  return tokenized"
      ],
      "metadata": {
        "id": "iQDzK_9IFZmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "tokenized_train = [ tokenize(text,label) for text,label in tqdm(zip(texts_train,labels_train)) ]\n",
        "tokenized_val = [ tokenize(text,label) for text,label in tqdm(zip(texts_val,labels_val)) ]"
      ],
      "metadata": {
        "id": "BpjXc558F5jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up a custom model\n",
        "\n",
        "We'll also use a custom model. This is a model that encodes the text using a BERT model, then takes the CLS vectors and puts them through one final layer to get two outputs."
      ],
      "metadata": {
        "id": "FCvxLt9PMIwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "import torch.nn as nn\n",
        "\n",
        "class ClassifierModel(torch.nn.Module):\n",
        "\tdef __init__(self, model_name):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.bert_model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "\t\tself.linear = nn.Linear(self.bert_model.config.hidden_size, 2)\n",
        "\n",
        "\tdef forward(self, input_ids, attention_mask):\n",
        "\t\tbert_output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "\t\tcls_vectors = bert_output.last_hidden_state[:,0,:]\n",
        "\n",
        "\t\toutput = self.linear(cls_vectors)\n",
        "\n",
        "\t\treturn output"
      ],
      "metadata": {
        "id": "E6EfmFliKPG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model and send it to the GPU:"
      ],
      "metadata": {
        "id": "0Kk-hF0hOvKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "\n",
        "model = ClassifierModel('bert-base-uncased')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "HOcruD5uLNEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Figuring out batches of data\n",
        "\n",
        "One of the fiddly bits of doing a training loop yourself is getting the data into nice batches. It's slow and often gives poor classification performance to train one sample at a time. So we'd like to put through a bunch together (e.g. 8 or 16 as common batch sizes). But our data can be different sizes. For instance, here is a short example:"
      ],
      "metadata": {
        "id": "zABSeyzWMCni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train[11]"
      ],
      "metadata": {
        "id": "Gz2_OsL7JvNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here's a slightly longer one:"
      ],
      "metadata": {
        "id": "mrpXbajJPCtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train[27]"
      ],
      "metadata": {
        "id": "TgUZoBOrKCVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And you can imagine that there are a lot of varied size. One way to solve this is to get the tokenizer to do padding for you, so that every sample is the same length (e.g. 512). That may be the most straightforward way. However, you may then be storing lots and lots of zeros in memory.\n",
        "\n",
        "An alternative way is to do padding and grouping on the fly. We'll do that.\n",
        "\n",
        "Let's say we've got a batch of 8 samples as below"
      ],
      "metadata": {
        "id": "1NGsTd2LPE1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenized_train[0:8]"
      ],
      "metadata": {
        "id": "lXiAh5ZBJjWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "They've all got different sizes ðŸ˜ž"
      ],
      "metadata": {
        "id": "15uC7KZ9Po_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[ x['input_ids'].shape[1] for x in batch ]"
      ],
      "metadata": {
        "id": "wRhIDeb9PiFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a function that does some padding for us with the [pad_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html) function."
      ],
      "metadata": {
        "id": "ZL1ac42XPw8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def custom_collate(batch):\n",
        "  output = {}\n",
        "  feature_names = [ f for f in batch[0].keys() ]\n",
        "  for feature_name in feature_names:\n",
        "    combined = [ b[feature_name][0,:] for b in batch ]\n",
        "    padded = pad_sequence(combined,batch_first=True)\n",
        "    assert padded.shape[0] == len(batch)\n",
        "    output[feature_name] = padded\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "c5CELA80JNuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if we run this on the batch, it groups and pads each of the sub-parts."
      ],
      "metadata": {
        "id": "nl_TZx0sP5as"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collated = custom_collate(batch)\n",
        "collated"
      ],
      "metadata": {
        "id": "kgSm_gG7KV3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now each part are all tensors, ready to be passed to BERT, etc."
      ],
      "metadata": {
        "id": "q2lvAMdHQBwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collated['input_ids'].shape"
      ],
      "metadata": {
        "id": "7G_yoQx8Ms4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing some hyperparameters\n",
        "\n",
        "We're getting close to some training. Let's pick a few hyperparameters. These can be optimised with Weights & Biases or an equivalent library."
      ],
      "metadata": {
        "id": "Sr3eL8rGMVfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 4"
      ],
      "metadata": {
        "id": "nFchtM8yJlhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's create a DataLoader and give it the `custom_collate` function that we used before. It manages the size of batches and shuffling data as well which is important for training."
      ],
      "metadata": {
        "id": "zYNFyNtvQYFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(tokenized_train, batch_size=batch_size, collate_fn=custom_collate, shuffle=True)\n",
        "val_loader = DataLoader(tokenized_val, batch_size=batch_size, collate_fn=custom_collate, shuffle=False)"
      ],
      "metadata": {
        "id": "W-Tt94AGJjZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we set up which optimizer we'll use (with the learning rate) as well as the loss function that we'll use to compare the model outputs to our targets."
      ],
      "metadata": {
        "id": "2RqCqgxGQoGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_func = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "MO6WWhu4Lenm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training time\n",
        "\n",
        "And here is the big training loop. It first iterates through the training set, updates the model and calculates the loss. Then it iterates through the validation data and calculates the loss on it."
      ],
      "metadata": {
        "id": "x-3geh8HQvJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training...\")\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  train_loss = 0.0\n",
        "\n",
        "  for batch in tqdm(train_loader):\n",
        "    batch = { k:v.to(device) for k,v in batch.items() }\n",
        "\n",
        "    # Zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(batch['input_ids'], batch['attention_mask'])\n",
        "    loss = loss_func(outputs, batch['label'].reshape(-1))\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "  train_loss /= len(train_loader)\n",
        "\n",
        "  # Validation after each epoch\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(val_loader):\n",
        "      batch = { k:v.to(device) for k,v in batch.items() }\n",
        "\n",
        "      # Forward pass and compute loss\n",
        "      outputs = model(batch['input_ids'], batch['attention_mask'])\n",
        "      loss = loss_func(outputs, batch['label'].reshape(-1))\n",
        "      val_loss += loss.item()\n",
        "\n",
        "  val_loss /= len(val_loader)\n",
        "\n",
        "  print(f\"{epoch=} {train_loss=:.4f} {val_loss=:.4f}\")"
      ],
      "metadata": {
        "id": "rtALh9pqH_nL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}