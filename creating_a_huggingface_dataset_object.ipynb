{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Colab Badge Link](https://img.shields.io/badge/open-in%20colab-blue)](https://colab.research.google.com/github/Glasgow-AI4BioMed/tutorials/blob/main/creating_a_huggingface_dataset_object.ipynb)\n",
        "\n",
        "## Example code for creating a HuggingFace Dataset and DatasetDict object\n",
        "\n",
        "HuggingFace often uses their own [Dataset](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset) and [DatasetDict](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetDict) objects in their tutorials as wrappers for various datasets. For example, it is used in their [token classification tutorial](https://huggingface.co/docs/transformers/tasks/token_classification). The API documentation for the two classes can be a little confusing. Hence it can be a bit fiddly to create your own dataset. So here is some example code.\n",
        "\n",
        "Before we get stuck in, we need to"
      ],
      "metadata": {
        "id": "2uEJGk_YVB_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies\n",
        "\n",
        "If needed, you could install the [datasets package](https://huggingface.co/docs/datasets/index) with the command below.\n",
        "\n",
        "```\n",
        "pip install datasets\n",
        "```"
      ],
      "metadata": {
        "id": "Ipt8H_s2Fpbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating datasets\n",
        "\n",
        "Let's make up some arbitrary data (which we assume we've got from somewhere else). Here the data has already been tokenized and comes with associated named entity (NER) tags - which in this case are nonsense. The data could also be untokenized, then the following code needs to do the tokenization."
      ],
      "metadata": {
        "id": "rJRCsiRpV50B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_tokens  = [ ['this', 'is', 'a', 'sentence'], ['this', 'is', 'an' 'another', 'sentence'], ['look', 'a', 'third', 'sentence'] ]\n",
        "training_nertags = [ ['O',    'O',  'O', 'B-WORD'],   ['O',    'O',  'O', 'O',       'B-WORD'],   ['O',    'O',  'O',    'B-WORD'] ]\n",
        "\n",
        "validation_tokens  = [ ['this', 'is', 'a', 'sentence', 'in', 'the', 'validation', 'set' ] ]\n",
        "validation_nertags = [ ['O',    'O',  'O', 'B-WORD',   'O',  'O',   'O',          'O'] ]"
      ],
      "metadata": {
        "id": "Ses1C4vETDf7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below creates a Dataset object for the training and validation data above."
      ],
      "metadata": {
        "id": "JuhbRLuZWLvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_dict({'tokens':training_tokens, 'ner_tags':training_nertags})\n",
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg4iW6LcUTuq",
        "outputId": "415e2044-5c04-4d68-e2a0-da160dfcf0e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['tokens', 'ner_tags'],\n",
              "    num_rows: 3\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And do the same for the validation dataset"
      ],
      "metadata": {
        "id": "ccc2XDOWWYDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = Dataset.from_dict({'tokens':validation_tokens, 'ner_tags':validation_nertags})\n",
        "validation_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ptMe3E1WXTA",
        "outputId": "0bd79c94-19b3-4e54-c7f6-8657c2f70a89"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['tokens', 'ner_tags'],\n",
              "    num_rows: 1\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, HuggingFace tutorials use a single object that combines all the data splits, so you can easily reference the training or validation parts. This uses a Dataset Dict object like below."
      ],
      "metadata": {
        "id": "rYUixl2aWP6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "dataset = DatasetDict()\n",
        "dataset['train'] = train_dataset\n",
        "dataset['validation'] = validation_dataset\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVHSk9VtUkF2",
        "outputId": "8b82b109-17a3-4003-e55e-80b93ef2e409"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tokens', 'ner_tags'],\n",
              "        num_rows: 3\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['tokens', 'ner_tags'],\n",
              "        num_rows: 1\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}