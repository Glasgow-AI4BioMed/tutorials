{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Colab Badge Link](https://img.shields.io/badge/open-in%20colab-blue)](https://colab.research.google.com/github/Glasgow-AI4BioMed/tutorials/blob/main/creating_a_huggingface_dataset_object.ipynb)\n",
        "\n",
        "# Creating a Hugging Face Dataset and DatasetDict object\n",
        "\n",
        "Hugging Face often uses their own [Dataset](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset) and [DatasetDict](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.DatasetDict) objects in their tutorials as wrappers for various datasets. For example, it is used in their [token classification tutorial](https://huggingface.co/docs/transformers/tasks/token_classification). The API documentation for the two classes can be a little confusing. Hence it can be a bit fiddly to create your own dataset. So here is some example code.\n"
      ],
      "metadata": {
        "id": "2uEJGk_YVB_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies\n",
        "\n",
        "If needed, you could install the [datasets package](https://huggingface.co/docs/datasets/index) with the command below.\n",
        "\n",
        "```\n",
        "pip install datasets\n",
        "```"
      ],
      "metadata": {
        "id": "Ipt8H_s2Fpbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating datasets\n",
        "\n",
        "Let's make up some arbitrary data (which we assume we've got from somewhere else). Here the data has already been tokenized and comes with associated named entity (NER) tags - which in this case are nonsense. The data could also be untokenized, then the following code needs to do the tokenization."
      ],
      "metadata": {
        "id": "rJRCsiRpV50B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [\n",
        "  {'tokens': ['this', 'is', 'a', 'sentence'], 'ner_tags': ['O', 'O', 'O', 'B-WORD']},\n",
        "  {'tokens': ['this', 'is', 'ananother', 'sentence'], 'ner_tags': ['O', 'O', 'O', 'O', 'B-WORD']},\n",
        "  {'tokens': ['look', 'a', 'third', 'sentence'], 'ner_tags': ['O', 'O', 'O', 'B-WORD']}\n",
        "]\n",
        "\n",
        "validation_data = [\n",
        "    {'tokens': ['this', 'is', 'a', 'sentence', 'in', 'the', 'validation', 'set'],\n",
        "     'ner_tags': ['O', 'O', 'O', 'B-WORD', 'O', 'O', 'O', 'O']}\n",
        "]"
      ],
      "metadata": {
        "id": "EkNXvfI3ZvS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below creates a Dataset object for the training and validation data above using the `.from_list` method."
      ],
      "metadata": {
        "id": "JuhbRLuZWLvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_list(training_data)\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "JLWoACGtaIVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset = Dataset.from_list(validation_data)\n",
        "validation_dataset"
      ],
      "metadata": {
        "id": "SR0W8cgXaX74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, if you have the fields stored as separate lists, you could create the dataset using the `.from_dict` method."
      ],
      "metadata": {
        "id": "iFiCnXmhacPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_tokens  = [ ['this', 'is', 'a', 'sentence'], ['this', 'is', 'an' 'another', 'sentence'], ['look', 'a', 'third', 'sentence'] ]\n",
        "training_nertags = [ ['O',    'O',  'O', 'B-WORD'],   ['O',    'O',  'O', 'O',       'B-WORD'],   ['O',    'O',  'O',    'B-WORD'] ]\n",
        "\n",
        "validation_tokens  = [ ['this', 'is', 'a', 'sentence', 'in', 'the', 'validation', 'set' ] ]\n",
        "validation_nertags = [ ['O',    'O',  'O', 'B-WORD',   'O',  'O',   'O',          'O'] ]"
      ],
      "metadata": {
        "id": "Ses1C4vETDf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_dict({'tokens':training_tokens, 'ner_tags':training_nertags})\n",
        "validation_dataset = Dataset.from_dict({'tokens':validation_tokens, 'ner_tags':validation_nertags})\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "fg4iW6LcUTuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, Hugging Face tutorials use a single object that combines all the data splits, so you can easily reference the training or validation parts. This uses a Dataset Dict object like below."
      ],
      "metadata": {
        "id": "rYUixl2aWP6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "dataset = DatasetDict()\n",
        "dataset['train'] = train_dataset\n",
        "dataset['validation'] = validation_dataset\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "sVHSk9VtUkF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benefits\n",
        "\n",
        "Why is this data structure useful? It allows you to split the data by the samples or by the fields.\n",
        "\n",
        "So, we could get samples by their index:"
      ],
      "metadata": {
        "id": "lHh_XVvfeJmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][0]"
      ],
      "metadata": {
        "id": "KtM-pdSEeMsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But we could already do that with the previous structure:"
      ],
      "metadata": {
        "id": "8xql4v7Wen_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data[0]"
      ],
      "metadata": {
        "id": "QLnTNwDUeoFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's new is that you can select an individual field (without having to do anything special):"
      ],
      "metadata": {
        "id": "0fKwjjcOeyPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']['tokens']"
      ],
      "metadata": {
        "id": "z4C66u8AePGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying a function across the dataset\n",
        "\n",
        "Another nice thing is that the structure works well with map functionality.\n",
        "\n",
        "Let's make up a text dataset with `label` and `text` fields:"
      ],
      "metadata": {
        "id": "ASDUi3IKdJQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = [\n",
        "    { 'label':'positive', 'text': \"The restaurant was great\" },\n",
        "    { 'label':'negative', 'text': \"Worst meal we've ever had\" },\n",
        "]\n",
        "\n",
        "val_samples = [\n",
        "    { 'label':'positive', 'text': \"A highlight of the trip\" },\n",
        "    { 'label':'negative', 'text': \"I'm still recovering.\" },\n",
        "]"
      ],
      "metadata": {
        "id": "xg8dzwRUdXHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put it into the datasets structure:"
      ],
      "metadata": {
        "id": "qPTJJCHjfJMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DatasetDict()\n",
        "dataset['train'] = Dataset.from_list(train_samples)\n",
        "dataset['validation'] = Dataset.from_list(val_samples)"
      ],
      "metadata": {
        "id": "yDLdV-jHdzYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common step involves running a tokenizer on the text using a helper function as below."
      ],
      "metadata": {
        "id": "4FiL4oYxfMyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenizer_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)"
      ],
      "metadata": {
        "id": "OPohiU30c7Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you can use the `.map` method on a dataset as below."
      ],
      "metadata": {
        "id": "eX9T7ppVfTqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = dataset['train'].map(tokenizer_function)\n",
        "tokenized_train"
      ],
      "metadata": {
        "id": "E6yDeOaweFDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or better yet, run it across the whole dataset including the different splits (train and validation in this case):"
      ],
      "metadata": {
        "id": "kXuC5RR7fcFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(tokenizer_function)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "39ZHQLUEd8Dq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}